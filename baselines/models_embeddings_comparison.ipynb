{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from feature_extraction import *\n",
    "import _pickle as cPickle\n",
    "from models import *\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of 200000 tweets for the training set...\n",
      "Processing tweet 1000/200000\n",
      "Processing tweet 2000/200000\n",
      "Processing tweet 3000/200000\n",
      "Processing tweet 4000/200000\n",
      "Processing tweet 5000/200000\n",
      "Processing tweet 6000/200000\n",
      "Processing tweet 7000/200000\n",
      "Processing tweet 8000/200000\n",
      "Processing tweet 9000/200000\n",
      "Processing tweet 10000/200000\n",
      "Processing tweet 11000/200000\n",
      "Processing tweet 12000/200000\n",
      "Processing tweet 13000/200000\n",
      "Processing tweet 14000/200000\n",
      "Processing tweet 15000/200000\n",
      "Processing tweet 16000/200000\n",
      "Processing tweet 17000/200000\n",
      "Processing tweet 18000/200000\n",
      "Processing tweet 19000/200000\n",
      "Processing tweet 20000/200000\n",
      "Processing tweet 21000/200000\n",
      "Processing tweet 22000/200000\n",
      "Processing tweet 23000/200000\n",
      "Processing tweet 24000/200000\n",
      "Processing tweet 25000/200000\n",
      "Processing tweet 26000/200000\n",
      "Processing tweet 27000/200000\n",
      "Processing tweet 28000/200000\n",
      "Processing tweet 29000/200000\n",
      "Processing tweet 30000/200000\n",
      "Processing tweet 31000/200000\n",
      "Processing tweet 32000/200000\n",
      "Processing tweet 33000/200000\n",
      "Processing tweet 34000/200000\n",
      "Processing tweet 35000/200000\n",
      "Processing tweet 36000/200000\n",
      "Processing tweet 37000/200000\n",
      "Processing tweet 38000/200000\n",
      "Processing tweet 39000/200000\n",
      "Processing tweet 40000/200000\n",
      "Processing tweet 41000/200000\n",
      "Processing tweet 42000/200000\n",
      "Processing tweet 43000/200000\n",
      "Processing tweet 44000/200000\n",
      "Processing tweet 45000/200000\n",
      "Processing tweet 46000/200000\n",
      "Processing tweet 47000/200000\n",
      "Processing tweet 48000/200000\n",
      "Processing tweet 49000/200000\n",
      "Processing tweet 50000/200000\n",
      "Processing tweet 51000/200000\n",
      "Processing tweet 52000/200000\n",
      "Processing tweet 53000/200000\n",
      "Processing tweet 54000/200000\n",
      "Processing tweet 55000/200000\n",
      "Processing tweet 56000/200000\n",
      "Processing tweet 57000/200000\n",
      "Processing tweet 58000/200000\n",
      "Processing tweet 59000/200000\n",
      "Processing tweet 60000/200000\n",
      "Processing tweet 61000/200000\n",
      "Processing tweet 62000/200000\n",
      "Processing tweet 63000/200000\n",
      "Processing tweet 64000/200000\n",
      "Processing tweet 65000/200000\n",
      "Processing tweet 66000/200000\n",
      "Processing tweet 67000/200000\n",
      "Processing tweet 68000/200000\n",
      "Processing tweet 69000/200000\n",
      "Processing tweet 70000/200000\n",
      "Processing tweet 71000/200000\n",
      "Processing tweet 72000/200000\n",
      "Processing tweet 73000/200000\n",
      "Processing tweet 74000/200000\n",
      "Processing tweet 75000/200000\n",
      "Processing tweet 76000/200000\n",
      "Processing tweet 77000/200000\n",
      "Processing tweet 78000/200000\n",
      "Processing tweet 79000/200000\n",
      "Processing tweet 80000/200000\n",
      "Processing tweet 81000/200000\n",
      "Processing tweet 82000/200000\n",
      "Processing tweet 83000/200000\n",
      "Processing tweet 84000/200000\n",
      "Processing tweet 85000/200000\n",
      "Processing tweet 86000/200000\n",
      "Processing tweet 87000/200000\n",
      "Processing tweet 88000/200000\n",
      "Processing tweet 89000/200000\n",
      "Processing tweet 90000/200000\n",
      "Processing tweet 91000/200000\n",
      "Processing tweet 92000/200000\n",
      "Processing tweet 93000/200000\n",
      "Processing tweet 94000/200000\n",
      "Processing tweet 95000/200000\n",
      "Processing tweet 96000/200000\n",
      "Processing tweet 97000/200000\n",
      "Processing tweet 98000/200000\n",
      "Processing tweet 99000/200000\n",
      "Processing tweet 100000/200000\n",
      "Processing tweet 101000/200000\n",
      "Processing tweet 102000/200000\n",
      "Processing tweet 103000/200000\n",
      "Processing tweet 104000/200000\n",
      "Processing tweet 105000/200000\n",
      "Processing tweet 106000/200000\n",
      "Processing tweet 107000/200000\n",
      "Processing tweet 108000/200000\n",
      "Processing tweet 109000/200000\n",
      "Processing tweet 110000/200000\n",
      "Processing tweet 111000/200000\n",
      "Processing tweet 112000/200000\n",
      "Processing tweet 113000/200000\n",
      "Processing tweet 114000/200000\n",
      "Processing tweet 115000/200000\n",
      "Processing tweet 116000/200000\n",
      "Processing tweet 117000/200000\n",
      "Processing tweet 118000/200000\n",
      "Processing tweet 119000/200000\n",
      "Processing tweet 120000/200000\n",
      "Processing tweet 121000/200000\n",
      "Processing tweet 122000/200000\n",
      "Processing tweet 123000/200000\n",
      "Processing tweet 124000/200000\n",
      "Processing tweet 125000/200000\n",
      "Processing tweet 126000/200000\n",
      "Processing tweet 127000/200000\n",
      "Processing tweet 128000/200000\n",
      "Processing tweet 129000/200000\n",
      "Processing tweet 130000/200000\n",
      "Processing tweet 131000/200000\n",
      "Processing tweet 132000/200000\n",
      "Processing tweet 133000/200000\n",
      "Processing tweet 134000/200000\n",
      "Processing tweet 135000/200000\n",
      "Processing tweet 136000/200000\n",
      "Processing tweet 137000/200000\n",
      "Processing tweet 138000/200000\n",
      "Processing tweet 139000/200000\n",
      "Processing tweet 140000/200000\n",
      "Processing tweet 141000/200000\n",
      "Processing tweet 142000/200000\n",
      "Processing tweet 143000/200000\n",
      "Processing tweet 144000/200000\n",
      "Processing tweet 145000/200000\n",
      "Processing tweet 146000/200000\n",
      "Processing tweet 147000/200000\n",
      "Processing tweet 148000/200000\n",
      "Processing tweet 149000/200000\n",
      "Processing tweet 150000/200000\n",
      "Processing tweet 151000/200000\n",
      "Processing tweet 152000/200000\n",
      "Processing tweet 153000/200000\n",
      "Processing tweet 154000/200000\n",
      "Processing tweet 155000/200000\n",
      "Processing tweet 156000/200000\n",
      "Processing tweet 157000/200000\n",
      "Processing tweet 158000/200000\n",
      "Processing tweet 159000/200000\n",
      "Processing tweet 160000/200000\n",
      "Processing tweet 161000/200000\n",
      "Processing tweet 162000/200000\n",
      "Processing tweet 163000/200000\n",
      "Processing tweet 164000/200000\n",
      "Processing tweet 165000/200000\n",
      "Processing tweet 166000/200000\n",
      "Processing tweet 167000/200000\n",
      "Processing tweet 168000/200000\n",
      "Processing tweet 169000/200000\n",
      "Processing tweet 170000/200000\n",
      "Processing tweet 171000/200000\n",
      "Processing tweet 172000/200000\n",
      "Processing tweet 173000/200000\n",
      "Processing tweet 174000/200000\n",
      "Processing tweet 175000/200000\n",
      "Processing tweet 176000/200000\n",
      "Processing tweet 177000/200000\n",
      "Processing tweet 178000/200000\n",
      "Processing tweet 179000/200000\n",
      "Processing tweet 180000/200000\n",
      "Processing tweet 181000/200000\n",
      "Processing tweet 182000/200000\n",
      "Processing tweet 183000/200000\n",
      "Processing tweet 184000/200000\n",
      "Processing tweet 185000/200000\n",
      "Processing tweet 186000/200000\n",
      "Processing tweet 187000/200000\n",
      "Processing tweet 188000/200000\n",
      "Processing tweet 189000/200000\n",
      "Processing tweet 190000/200000\n",
      "Processing tweet 191000/200000\n",
      "Processing tweet 192000/200000\n",
      "Processing tweet 193000/200000\n",
      "Processing tweet 194000/200000\n",
      "Processing tweet 195000/200000\n",
      "Processing tweet 196000/200000\n",
      "Processing tweet 197000/200000\n",
      "Processing tweet 198000/200000\n",
      "Processing tweet 199000/200000\n",
      "Processing tweet 200000/200000\n",
      "Processing of 10000 tweets for the testing set...\n",
      "Processing tweet 1000/10000\n",
      "Processing tweet 2000/10000\n",
      "Processing tweet 3000/10000\n",
      "Processing tweet 4000/10000\n",
      "Processing tweet 5000/10000\n",
      "Processing tweet 6000/10000\n",
      "Processing tweet 7000/10000\n",
      "Processing tweet 8000/10000\n",
      "Processing tweet 9000/10000\n",
      "Processing tweet 10000/10000\n",
      "Tokenization of tweets...\n",
      "Found 82394 unique tokens.\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 29\n",
      "Average test sequence length: 26\n",
      "Pad sequences\n",
      "train_sequences shape: (200000, 60)\n",
      "test_sequences shape: (10000, 60)\n",
      "Shuffling of the training set...\n"
     ]
    }
   ],
   "source": [
    "save(\"train_pos.txt\", \"train_neg.txt\", False, -1, 2, 'embedding_hashtag_split/features_ngram_small.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, y, X_test, max_features, W] = cPickle.load(open(\"embedding_hashtag_split/features_ngram_small.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.normalization.BatchNormalization at 0x7efdb2eaa080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n",
    "\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "\n",
    "Activation('relu')\n",
    "BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embeddings...\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(vocab):\n",
    "    max_rank = max(lex.rank+1 for lex in vocab if lex.has_vector)\n",
    "    vectors = np.ndarray((max_rank+1, vocab.vectors_length), dtype='float32')\n",
    "    for lex in vocab:\n",
    "        if lex.has_vector:\n",
    "            vectors[lex.rank + 1] = lex.vector\n",
    "    return vectors\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "vocab_nlp = spacy.load('en_core_web_lg', parser=False, tagger=False, entity=False)\n",
    "print('Preparing embeddings...')\n",
    "embeddings_spacy = get_embeddings(vocab_nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((684825, 300), 663394)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_spacy.shape, max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumpFeatures(False, False, False, None, 'monogram_features.dat')\n",
    "# [X_train_mono, y_mono, X_test_mono, max_features_mono] = cPickle.load(open(\"monogram_features.dat\", \"rb\"))\n",
    "\n",
    "\n",
    "# [X_train, y, X_test, max_features] = cPickle.load(open(\"bigram_features.dat\", \"rb\"))\n",
    "\n",
    "# dumpFeatures(True, True, False, None, 'bigram_features_full.dat')\n",
    "# [X_train, y, X_test, max_features] = cPickle.load(open(\"bigram_features_full.dat\", \"rb\"))\n",
    "\n",
    "# dumpFeatures(False, False, True, None, 'features_pretrained_small.dat')\n",
    "# [X_train, y, X_test, max_features, W] = cPickle.load(open(\"features_pretrained_small.dat\", \"rb\"))\n",
    "\n",
    "\n",
    "# dumpFeatures(False, True, True, None, 'features_pretrained_bigram_small.dat')\n",
    "# [X_train, y, X_test, max_features, W] = cPickle.load(open(\"features_pretrained_bigram_small.dat\", \"rb\"))\n",
    "\n",
    "\n",
    "# dumpFeatures(True, False, True, None, 'features_pretrained_full.dat')\n",
    "[X_train, y, X_test, max_features, W] = cPickle.load(open(\"features_pretrained_full.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1010: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1153: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "[X_train, y, X_test, max_features, W] = cPickle.load(open(\"features_pretrained_small.dat\", \"rb\"))\n",
    "model = build_model2(max_features+1, W.shape[1], X_train.shape[1], embeddings=W, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 30), (200000,), (10000, 30), 96338, (96339, 200))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y.shape, X_test.shape, max_features, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808682, 50, 200000, 60)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features+1, 50, X_train.shape[0], X_train.shape[1]\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96339, 200)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 30, 50)            4816950   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 4,817,001\n",
      "Trainable params: 4,817,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1_2 = build_model1(X_train_mono, max_features_mono+1, 50, X_train_mono.shape[1])\n",
    "model1_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "   384/180000 [..............................] - ETA: 61s - loss: 0.3664 - acc: 0.8307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000/180000 [==============================] - 49s - loss: 0.3593 - acc: 0.8436 - val_loss: 0.3941 - val_acc: 0.8265\n",
      "Epoch 2/2\n",
      "180000/180000 [==============================] - 49s - loss: 0.3417 - acc: 0.8533 - val_loss: 0.3945 - val_acc: 0.8245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f842217ba90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_2.fit(X_train_mono, y_mono, validation_split=0.1, nb_epoch=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 30, 200)           19267800  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 19,268,001\n",
      "Trainable params: 19,268,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model1 = build_model1(X_train, max_features+1, 50, X_train.shape[1])\n",
    "model1 = build_model1(X_train, max_features+1, W.shape[1], X_train.shape[1], W)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "\r",
      "   128/180000 [..............................] - ETA: 227s - loss: 0.3889 - acc: 0.8281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000/180000 [==============================] - 181s - loss: 0.3497 - acc: 0.8486 - val_loss: 0.3949 - val_acc: 0.8256\n",
      "Epoch 2/2\n",
      "180000/180000 [==============================] - 182s - loss: 0.3313 - acc: 0.8572 - val_loss: 0.3971 - val_acc: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8422b27a20>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y, validation_split=0.1, nb_epoch=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1010: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1153: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 60, 200)           161736400 \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 56, 128)           128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 161,946,705\n",
      "Trainable params: 161,946,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model2(max_features+1, W.shape[1], X_train.shape[1], embeddings=W, trainable=True)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:105: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 161736400 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 1333s - loss: 0.3544 - acc: 0.8371 - val_loss: 0.3240 - val_acc: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3678f80b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y, validation_split=0.1, nb_epoch=1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9760/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "predicted_labels = model2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[predicted_labels >= 0.5] = 1\n",
    "predicted_labels[predicted_labels < 0.5] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-5e53859c77e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# raw_data_test.shape, np.arange(X_test.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "# raw_data_test.shape, np.arange(X_test.shape[0])\n",
    "predicted_labels, raw_data_test.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model2_small_dataset_1_epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "#             print(y_pred)\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "\n",
    "create_csv_submission(np.arange(1, X_test.shape[0]+1), predicted_labels, 'model2_small_dataset_1_epoch__2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data neg and pos with corresponding label\n",
    "\n",
    "raw_data_neg = pd.read_csv('train_neg.txt', header=None, sep=\"\\n\", encoding='utf-8', names=['text'],\n",
    "                               error_bad_lines=True, warn_bad_lines=True, quoting=csv.QUOTE_NONE).drop_duplicates()\n",
    "# raw_data_neg['text'] = raw_data_neg['text']\n",
    "raw_data_neg['label'] = 0\n",
    "# raw_data_neg = raw_data_neg[:5000]\n",
    "\n",
    "raw_data_pos = pd.read_csv('train_pos.txt', header=None, sep=\"\\n\", encoding='utf-8', names=['text'],\n",
    "                               error_bad_lines=True, warn_bad_lines=True, quoting=csv.QUOTE_NONE).drop_duplicates()\n",
    "raw_data_pos['label'] = 1\n",
    "# raw_data_pos = raw_data_pos[:5000]\n",
    "\n",
    "\n",
    "raw_data_train = pd.concat([raw_data_neg, raw_data_pos], ignore_index=True)\n",
    "\n",
    "indices = np.arange(raw_data_train.shape[0])\n",
    "np.random.shuffle(indices), raw_data_train.shape[0]\n",
    "\n",
    "raw_data_train.loc[indices].text, raw_data_train.loc[indices].label\n",
    "\n",
    "raw_data_train.text = raw_data_train.loc[indices].text.values\n",
    "raw_data_train.label = raw_data_train.loc[indices].label.values\n",
    "\n",
    "\n",
    "\n",
    "# raw_data_test = pd.read_csv('test_data.txt', header=None, sep=\"\\n\", encoding='utf-8', names=['text'],\n",
    "#                                error_bad_lines=False, warn_bad_lines=False, quoting=csv.QUOTE_NONE).drop_duplicates()\n",
    "\n",
    "\n",
    "# raw_data_test['id'], raw_data_test['text'] = raw_data_test['text'].apply(lambda x: x.split(',')[0]), raw_data_test['text'].apply(lambda x: ','.join(x.split(',')[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(raw_data_train['text'], batch_size=1000, n_threads=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    X = np.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            if token.has_vector and not token.is_punct and not token.is_space:\n",
    "                X[i, j] = token.rank + 1\n",
    "                j += 1\n",
    "                if j >= max_length:\n",
    "                    break\n",
    "    return X\n",
    "\n",
    "docs_extracted = extract_features(docs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   172,  21899,   1944, ...,      0,      0,      0],\n",
       "       [   106,    432,     32, ...,      0,      0,      0],\n",
       "       [ 47573,     44,     10, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [ 34332, 447228,   1350, ...,      0,      0,      0],\n",
       "       [     4,    376,     12, ...,      0,      0,      0],\n",
       "       [ 69700,  31588, 447228, ...,      0,      0,      0]], dtype=int32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344233, (684825, 300), 100)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape, raw_data_train.label.shape\n",
    "features_train[0]\n",
    "len(vocab_nlp.vocab), embeddings_spacy.shape, docs_extracted.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_spacy.shape[0], embeddings_spacy.shape[1], features_train.shape[1]\n",
    "embeddings_spacy.shape, docs_extracted.shape[1]\n",
    "\n",
    "embeddings_spacy is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(max_features+1, size2, input_length=X_train.shape[1]))\n",
    "model.add(Embedding(embeddings_spacy.shape[0], embeddings_spacy.shape[1], input_length=docs_extracted.shape[1]))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wat\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 100, 300)          205447500 \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_20  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 205,447,801\n",
      "Trainable params: 301\n",
      "Non-trainable params: 205,447,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1_2 = build_model1(features_train, embeddings_spacy.shape[0], embeddings_spacy.shape[1], docs_extracted.shape[1], embeddings_spacy)\n",
    "model1_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163188 samples, validate on 18133 samples\n",
      "Epoch 1/40\n",
      "  5632/163188 [>.............................] - ETA: 4s - loss: 0.4981 - acc: 0.7468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163188/163188 [==============================] - 4s - loss: 0.4998 - acc: 0.7529 - val_loss: 0.5021 - val_acc: 0.7499\n",
      "Epoch 2/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4995 - acc: 0.7532 - val_loss: 0.5017 - val_acc: 0.7508\n",
      "Epoch 3/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4992 - acc: 0.7536 - val_loss: 0.5016 - val_acc: 0.7505\n",
      "Epoch 4/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4990 - acc: 0.7533 - val_loss: 0.5012 - val_acc: 0.7513\n",
      "Epoch 5/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4988 - acc: 0.7538 - val_loss: 0.5011 - val_acc: 0.7508\n",
      "Epoch 6/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4986 - acc: 0.7543 - val_loss: 0.5009 - val_acc: 0.7505\n",
      "Epoch 7/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4984 - acc: 0.7540 - val_loss: 0.5006 - val_acc: 0.7512\n",
      "Epoch 8/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4981 - acc: 0.7544 - val_loss: 0.5005 - val_acc: 0.7513\n",
      "Epoch 9/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4980 - acc: 0.7547 - val_loss: 0.5003 - val_acc: 0.7510\n",
      "Epoch 10/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4978 - acc: 0.7548 - val_loss: 0.5002 - val_acc: 0.7517\n",
      "Epoch 11/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4976 - acc: 0.7549 - val_loss: 0.4999 - val_acc: 0.7510\n",
      "Epoch 12/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4974 - acc: 0.7549 - val_loss: 0.4998 - val_acc: 0.7517\n",
      "Epoch 13/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4973 - acc: 0.7554 - val_loss: 0.4996 - val_acc: 0.7514\n",
      "Epoch 14/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4971 - acc: 0.7553 - val_loss: 0.4995 - val_acc: 0.7523\n",
      "Epoch 15/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4969 - acc: 0.7553 - val_loss: 0.4994 - val_acc: 0.7527\n",
      "Epoch 16/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4968 - acc: 0.7557 - val_loss: 0.4992 - val_acc: 0.7517\n",
      "Epoch 17/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4967 - acc: 0.7558 - val_loss: 0.4991 - val_acc: 0.7528\n",
      "Epoch 18/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4965 - acc: 0.7557 - val_loss: 0.4989 - val_acc: 0.7525\n",
      "Epoch 19/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4964 - acc: 0.7559 - val_loss: 0.4988 - val_acc: 0.7526\n",
      "Epoch 20/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4963 - acc: 0.7559 - val_loss: 0.4988 - val_acc: 0.7525\n",
      "Epoch 21/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4961 - acc: 0.7561 - val_loss: 0.4986 - val_acc: 0.7530\n",
      "Epoch 22/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4960 - acc: 0.7562 - val_loss: 0.4985 - val_acc: 0.7535\n",
      "Epoch 23/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4959 - acc: 0.7562 - val_loss: 0.4984 - val_acc: 0.7530\n",
      "Epoch 24/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4958 - acc: 0.7563 - val_loss: 0.4983 - val_acc: 0.7535\n",
      "Epoch 25/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4957 - acc: 0.7563 - val_loss: 0.4982 - val_acc: 0.7537\n",
      "Epoch 26/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4956 - acc: 0.7564 - val_loss: 0.4980 - val_acc: 0.7535\n",
      "Epoch 27/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4955 - acc: 0.7562 - val_loss: 0.4980 - val_acc: 0.7540\n",
      "Epoch 28/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4954 - acc: 0.7565 - val_loss: 0.4983 - val_acc: 0.7540\n",
      "Epoch 29/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4953 - acc: 0.7563 - val_loss: 0.4978 - val_acc: 0.7543\n",
      "Epoch 30/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4952 - acc: 0.7566 - val_loss: 0.4979 - val_acc: 0.7542\n",
      "Epoch 31/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4951 - acc: 0.7568 - val_loss: 0.4977 - val_acc: 0.7529\n",
      "Epoch 32/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4950 - acc: 0.7570 - val_loss: 0.4975 - val_acc: 0.7539\n",
      "Epoch 33/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4949 - acc: 0.7567 - val_loss: 0.4975 - val_acc: 0.7542\n",
      "Epoch 34/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4948 - acc: 0.7568 - val_loss: 0.4974 - val_acc: 0.7543\n",
      "Epoch 35/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4947 - acc: 0.7569 - val_loss: 0.4973 - val_acc: 0.7540\n",
      "Epoch 36/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4947 - acc: 0.7568 - val_loss: 0.4972 - val_acc: 0.7539\n",
      "Epoch 37/40\n",
      "163188/163188 [==============================] - 5s - loss: 0.4946 - acc: 0.7569 - val_loss: 0.4972 - val_acc: 0.7546\n",
      "Epoch 38/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4945 - acc: 0.7572 - val_loss: 0.4971 - val_acc: 0.7544\n",
      "Epoch 39/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4944 - acc: 0.7571 - val_loss: 0.4971 - val_acc: 0.7538\n",
      "Epoch 40/40\n",
      "163188/163188 [==============================] - 4s - loss: 0.4944 - acc: 0.7573 - val_loss: 0.4970 - val_acc: 0.7541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb82d3a29e8>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1_2.fit(features_train, raw_data_train.label.values, validation_split=0.1, nb_epoch=3, batch_size=128, verbose=1)\n",
    "model1_2.fit(docs_extracted, raw_data_train.label.values, validation_split=0.1, nb_epoch=40, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "new_model = load_model('features_pretrained_full_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 200)           98542000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 128)           128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,752,305\n",
      "Trainable params: 98,752,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250000 samples, validate on 250000 samples\n",
      "Epoch 1/1\n",
      "   1408/2250000 [..............................] - ETA: 14564s - loss: 0.2585 - acc: 0.8878"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-33a61dd61e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_model.fit(X_train, y, validation_split=0.1, nb_epoch=1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 200)           98542000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 128)           128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 128)            82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,752,305\n",
      "Trainable params: 98,752,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9792/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "new_model.summary()\n",
    "\n",
    "predicted_labels = new_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[predicted_labels >= 0.5] = 1\n",
    "predicted_labels[predicted_labels < 0.5] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "#             print(y_pred)\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "\n",
    "create_csv_submission(np.arange(1, X_test.shape[0]+1), predicted_labels, 'model2_full_dataset_2_epochs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
